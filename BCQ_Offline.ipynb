{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4b13c9c-e52e-4f18-a674-a90b7a3edc3f",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 132\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[1;32m    131\u001b[0m     a \u001b[38;5;241m=\u001b[39m random_policy(s)\n\u001b[0;32m--> 132\u001b[0m     s2, r, done, _ \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;66;03m# Store as tensors directly\u001b[39;00m\n\u001b[1;32m    134\u001b[0m     s_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(s)\u001b[38;5;241m.\u001b[39mfloat()\n",
      "Cell \u001b[0;32mIn[1], line 71\u001b[0m, in \u001b[0;36mTradingEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     68\u001b[0m a \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(action)\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mclamp_min(\u001b[38;5;241m1e-6\u001b[39m)\n\u001b[1;32m     69\u001b[0m a \u001b[38;5;241m=\u001b[39m a \u001b[38;5;241m/\u001b[39m a\u001b[38;5;241m.\u001b[39msum()\n\u001b[0;32m---> 71\u001b[0m prices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprices_raw\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_step\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclamp_min\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1e-6\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m total_value \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mholdings \u001b[38;5;241m*\u001b[39m prices)\u001b[38;5;241m.\u001b[39msum() \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcash\n\u001b[1;32m     74\u001b[0m target_vals \u001b[38;5;241m=\u001b[39m a[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m*\u001b[39m total_value\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas_datareader import data as pdr\n",
    "import yfinance as yf\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# 0) SET DEVICE\n",
    "# --------------------------------------------------------\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# 1) OPTIMIZED TRADING ENVIRONMENT\n",
    "# --------------------------------------------------------\n",
    "class TradingEnv(gym.Env):\n",
    "    def __init__(self, prices, divs, vols, irate, initial_cash=10000, transaction_cost=1e-4):\n",
    "        super().__init__()\n",
    "        self.prices_raw = prices.to(device)\n",
    "        self.divs_raw = divs.to(device)\n",
    "        self.vols_raw = vols.to(device)\n",
    "        self.irate_raw = irate.to(device)\n",
    "        self.initial_cash = initial_cash\n",
    "        self.transaction_cost = transaction_cost\n",
    "        self.T, self.N = prices.shape\n",
    "\n",
    "        # Precompute normalization on GPU\n",
    "        self.mu_p = prices.mean(0).to(device)\n",
    "        self.sd_p = prices.std(0).add(1e-6).to(device)\n",
    "        self.mu_d = divs.mean(0).to(device)\n",
    "        self.sd_d = divs.std(0).add(1e-6).to(device)\n",
    "        self.mu_v = vols.mean(0).to(device)\n",
    "        self.sd_v = vols.std(0).add(1e-6).to(device)\n",
    "        self.mu_i = irate.mean().to(device)\n",
    "        self.sd_i = irate.std().add(1e-6).to(device)\n",
    "\n",
    "        self.action_space = gym.spaces.Box(0.0, 1.0, (self.N+1,), np.float32)\n",
    "        obs_dim = self.N*3 + 2\n",
    "        self.observation_space = gym.spaces.Box(-np.inf, np.inf, (obs_dim,), np.float32)\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        self.cash = float(self.initial_cash)\n",
    "        self.holdings = torch.zeros(self.N, device=device)\n",
    "        return self._get_obs()\n",
    "\n",
    "    def _get_obs(self):\n",
    "        t = self.current_step\n",
    "        p = (self.prices_raw[t] - self.mu_p) / self.sd_p\n",
    "        d = (self.divs_raw[t] - self.mu_d) / self.sd_d\n",
    "        v = (self.vols_raw[t] - self.mu_v) / self.sd_v\n",
    "        i = (self.irate_raw[t] - self.mu_i) / self.sd_i\n",
    "        obs = torch.cat([p, d, v, \n",
    "                        torch.tensor([self.cash], device=device), \n",
    "                        torch.tensor([i], device=device)])\n",
    "        return obs.float().cpu().numpy()\n",
    "\n",
    "    def step(self, action):\n",
    "        if self.current_step >= self.T - 1:\n",
    "            return self._get_obs(), 0.0, True, {}\n",
    "\n",
    "        a = torch.from_numpy(action).to(device).clamp_min(1e-6)\n",
    "        a = a / a.sum()\n",
    "\n",
    "        prices = self.prices_raw[self.current_step].clamp_min(1e-6)\n",
    "        total_value = (self.holdings * prices).sum() + self.cash\n",
    "\n",
    "        target_vals = a[:-1] * total_value\n",
    "        target_cash = a[-1] * total_value\n",
    "        new_holdings = target_vals / prices\n",
    "\n",
    "        turnover = (new_holdings - self.holdings).abs() * prices\n",
    "        cost = self.transaction_cost * turnover.sum()\n",
    "\n",
    "        next_prices = self.prices_raw[self.current_step+1].clamp_min(1e-6)\n",
    "        next_total = (new_holdings * next_prices).sum() + target_cash - cost\n",
    "        reward = (next_total / total_value - 1.0).item()\n",
    "\n",
    "        self.holdings = new_holdings\n",
    "        self.cash = (target_cash - cost).item()\n",
    "        self.current_step += 1\n",
    "        done = (self.current_step >= self.T - 1)\n",
    "\n",
    "        return self._get_obs(), reward, done, {}\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# 2) DATA LOADING WITH TENSOR CONVERSION\n",
    "# --------------------------------------------------------\n",
    "start = datetime.datetime(1990, 1, 1)\n",
    "end = datetime.datetime(2019, 12, 31)\n",
    "tickers = [\n",
    "    \"AAPL\", \"MSFT\", \"UNH\", \"LLY\", \"JPM\", \"JNJ\", \"XOM\", \"WMT\", \"PG\", \"CVX\",\n",
    "    \"HD\", \"MRK\", \"COST\", \"PEP\", \"ADBE\", \"KO\", \"BAC\", \"ORCL\", \"INTC\",\n",
    "    \"MCD\", \"ABT\", \"CSCO\", \"QCOM\", \"DHR\", \"NKE\", \"WFC\", \"TXN\", \"AMD\",\n",
    "    \"NEE\", \"AMGN\", \"PM\", \"HON\", \"UNP\", \"UPS\", \"MS\", \"LOW\", \"BA\", \"IBM\",\n",
    "    \"CAT\", \"MDT\", \"GS\", \"GE\", \"DE\", \"T\", \"LRCX\", \"ADI\", \"CI\", \"SYK\",\n",
    "    \"MU\", \"SCHW\", \"ADP\", \"MMC\", \"BDX\", \"PFE\", \"ADSK\", \"SO\", \"PGR\", \"TGT\",\n",
    "    \"AXP\", \"AON\", \"SLB\", \"CL\", \"APD\", \"AEP\", \"CSX\", \"F\", \"GM\", \"FDX\",\n",
    "    \"DG\", \"NSC\", \"ITW\"]\n",
    "\n",
    "df = yf.download(tickers, start=start, end=end, auto_adjust=False, actions=True, progress=False)\n",
    "\n",
    "# Convert to tensors immediately\n",
    "prices = torch.tensor(df['Close'].ffill().bfill().values.astype(np.float32), device=device)\n",
    "vols = torch.tensor(df['Volume'].ffill().bfill().values.astype(np.float32), device=device)\n",
    "divs = torch.tensor(df['Dividends'].fillna(0.0).values.astype(np.float32), device=device)\n",
    "\n",
    "macro = pdr.DataReader(['DGS10'], 'fred', start, end).reindex(df.index).ffill().bfill()\n",
    "irate = torch.tensor(macro['DGS10'].values.astype(np.float32), device=device)\n",
    "\n",
    "env = TradingEnv(prices, divs, vols, irate)\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# 3) TENSOR-BASED DATA COLLECTION\n",
    "# --------------------------------------------------------\n",
    "buffer = []\n",
    "def random_policy(state):\n",
    "    a = np.random.rand(env.N+1).astype(np.float32)\n",
    "    return a / a.sum()\n",
    "\n",
    "for _ in range(500):\n",
    "    s = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        a = random_policy(s)\n",
    "        s2, r, done, _ = env.step(a)\n",
    "        # Store as tensors directly\n",
    "        s_tensor = torch.from_numpy(s).float()\n",
    "        a_tensor = torch.from_numpy(a).float()\n",
    "        s2_tensor = torch.from_numpy(s2).float()\n",
    "        r_tensor = torch.tensor(r, dtype=torch.float32)\n",
    "        done_tensor = torch.tensor(done, dtype=torch.bool)\n",
    "        buffer.append((s_tensor, a_tensor, r_tensor, s2_tensor, done_tensor))\n",
    "        s = s2\n",
    "\n",
    "class TorchReplayDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "dataset = TorchReplayDataset(buffer)\n",
    "replay_loader = DataLoader(dataset, batch_size=256, shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# 4) CORRECTED MODEL ARCHITECTURES\n",
    "# --------------------------------------------------------\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, s_dim, a_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(s_dim+a_dim, 256), nn.ReLU(),\n",
    "            nn.Linear(256, 256), nn.ReLU(),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "    def forward(self, s, a):\n",
    "        return self.net(torch.cat([s,a], dim=1))\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, s_dim, a_dim, z_dim=10):\n",
    "        super().__init__()\n",
    "        self.z_dim = z_dim\n",
    "        self.enc = nn.Sequential(\n",
    "            nn.Linear(s_dim+a_dim, 256), nn.ReLU(),\n",
    "            nn.Linear(256, 256), nn.ReLU(),\n",
    "            nn.Linear(256, 2*z_dim)\n",
    "        )\n",
    "        self.dec = nn.Sequential(\n",
    "            nn.Linear(s_dim+z_dim, 256), nn.ReLU(),\n",
    "            nn.Linear(256, 256), nn.ReLU(),\n",
    "            nn.Linear(256, a_dim), nn.Sigmoid()  # Corrected activation\n",
    "        )\n",
    "    def encode(self, s, a):\n",
    "        mu_logstd = self.enc(torch.cat([s,a], dim=1))\n",
    "        mu, logstd = mu_logstd.chunk(2, dim=1)\n",
    "        return mu, logstd\n",
    "    def decode(self, s, z=None):\n",
    "        if z is None:\n",
    "            z = torch.randn(s.size(0), self.z_dim, device=s.device)\n",
    "        return self.dec(torch.cat([s,z], dim=1))\n",
    "    def forward(self, s, a):\n",
    "        mu, logstd = self.encode(s, a)\n",
    "        std = torch.exp(logstd.clamp(-4, 15))\n",
    "        z = mu + std * torch.randn_like(std)\n",
    "        return self.decode(s, z), mu, std\n",
    "\n",
    "class Perturbation(nn.Module):\n",
    "    def __init__(self, s_dim, a_dim, phi=0.05):\n",
    "        super().__init__()\n",
    "        self.phi = phi\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(s_dim+a_dim, 256), nn.ReLU(),\n",
    "            nn.Linear(256, a_dim), nn.Tanh()\n",
    "        )\n",
    "    def forward(self, s, a):\n",
    "        return self.phi*self.net(torch.cat([s,a], dim=1))\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# 5) OPTIMIZED BCQ AGENT\n",
    "# --------------------------------------------------------\n",
    "class BCQAgent:\n",
    "    def __init__(self, s_dim, a_dim, device):\n",
    "        self.device = device\n",
    "        self.action_dim = a_dim\n",
    "        self.vae = VAE(s_dim, a_dim).to(device)\n",
    "        self.q1 = QNetwork(s_dim, a_dim).to(device)\n",
    "        self.q2 = QNetwork(s_dim, a_dim).to(device)\n",
    "        self.q1_t = QNetwork(s_dim, a_dim).to(device)\n",
    "        self.q2_t = QNetwork(s_dim, a_dim).to(device)\n",
    "        self.pert = Perturbation(s_dim, a_dim).to(device)\n",
    "\n",
    "        self.q1_t.load_state_dict(self.q1.state_dict())\n",
    "        self.q2_t.load_state_dict(self.q2.state_dict())\n",
    "\n",
    "        self.opt_vae = optim.Adam(self.vae.parameters(), lr=1e-4)\n",
    "        self.opt_q1 = optim.Adam(self.q1.parameters(), lr=1e-4)\n",
    "        self.opt_q2 = optim.Adam(self.q2.parameters(), lr=1e-4)\n",
    "        self.opt_pert = optim.Adam(self.pert.parameters(), lr=1e-4)\n",
    "\n",
    "        self.discount = 0.995\n",
    "        self.tau = 0.005\n",
    "        self.lmbda = 0.75\n",
    "\n",
    "    def train_step(self, batch):\n",
    "        s, a, r, s2, done = [x.to(self.device) for x in batch]\n",
    "        r = r.unsqueeze(1)\n",
    "        done = done.unsqueeze(1)\n",
    "\n",
    "        # VAE Update\n",
    "        recon, mu, std = self.vae(s, a)\n",
    "        loss_recon = nn.MSELoss()(recon, a)\n",
    "        loss_kl = -0.5*(1 + torch.log(std**2) - mu**2 - std**2).sum(1).mean()\n",
    "        self.opt_vae.zero_grad()\n",
    "        (loss_recon + 0.5*loss_kl).backward()\n",
    "        self.opt_vae.step()\n",
    "\n",
    "        # Critic Update\n",
    "        with torch.no_grad():\n",
    "            s2_rep = s2.unsqueeze(1).repeat(1, 5, 1).view(-1, s2.size(1))\n",
    "            a_dec = self.vae.decode(s2_rep)\n",
    "            a_pert = self.pert(s2_rep, a_dec)\n",
    "            a_perturbed = torch.clamp(a_dec + a_pert, 0.0, 1.0)\n",
    "            a_perturbed = a_perturbed / a_perturbed.sum(dim=1, keepdim=True)\n",
    "            \n",
    "            q1_t = self.q1_t(s2_rep, a_perturbed).view(s2.size(0), 5, 1)\n",
    "            q2_t = self.q2_t(s2_rep, a_perturbed).view(s2.size(0), 5, 1)\n",
    "            c_star = self.lmbda * q1_t.min(1)[0] + (1 - self.lmbda) * q1_t.max(1)[0]\n",
    "            target_q = r + (1 - done) * self.discount * c_star\n",
    "\n",
    "        q1 = self.q1(s, a)\n",
    "        loss_q1 = nn.MSELoss()(q1, target_q)\n",
    "        self.opt_q1.zero_grad(); loss_q1.backward(); self.opt_q1.step()\n",
    "\n",
    "        q2 = self.q2(s, a)\n",
    "        loss_q2 = nn.MSELoss()(q2, target_q)\n",
    "        self.opt_q2.zero_grad(); loss_q2.backward(); self.opt_q2.step()\n",
    "\n",
    "        # Perturbation Update\n",
    "        a_p = self.vae.decode(s)\n",
    "        a_p = a_p + self.pert(s, a_p)\n",
    "        a_p = torch.clamp(a_p, 0.0, 1.0)\n",
    "        a_p = a_p / a_p.sum(dim=1, keepdim=True)\n",
    "        loss_pert = -self.q1(s, a_p).mean()\n",
    "        self.opt_pert.zero_grad(); loss_pert.backward(); self.opt_pert.step()\n",
    "\n",
    "        # Target Updates\n",
    "        for param, target_param in zip(self.q1.parameters(), self.q1_t.parameters()):\n",
    "            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "        for param, target_param in zip(self.q2.parameters(), self.q2_t.parameters()):\n",
    "            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "\n",
    "    def act(self, state, epsilon=0.0):\n",
    "        if np.random.rand() < epsilon:\n",
    "            a = np.random.rand(self.action_dim).astype(np.float32)\n",
    "            return a / (a.sum() + 1e-6)\n",
    "        \n",
    "        s = torch.FloatTensor(state).to(self.device).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            s_rep = s.repeat(10, 1)\n",
    "            a_samp = self.vae.decode(s_rep)\n",
    "            a_pert = self.pert(s_rep, a_samp)\n",
    "            a_perturbed = torch.clamp(a_samp + a_pert, 0.0, 1.0)\n",
    "            a_perturbed = a_perturbed / (a_perturbed.sum(dim=1, keepdim=True) + 1e-6)\n",
    "            q_vals = self.q1(s_rep, a_perturbed)\n",
    "            best = a_perturbed[q_vals.argmax()].cpu().numpy()\n",
    "        return best\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# 6) TRAINING LOOP WITH FIXED DATA HANDLING\n",
    "# --------------------------------------------------------\n",
    "agent = BCQAgent(env.observation_space.shape[0], env.action_space.shape[0], device)\n",
    "num_epochs = 50\n",
    "history = []\n",
    "epsilon_start, epsilon_final = 1.0, 0.01\n",
    "decay = np.log(epsilon_final/epsilon_start) / (num_epochs * (len(dataset)//256))\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    for batch in replay_loader:\n",
    "        agent.train_step(batch)\n",
    "\n",
    "    epsilon = epsilon_start * np.exp(decay * epoch * (len(dataset)//256))\n",
    "    returns = []\n",
    "    for _ in range(10):\n",
    "        s, done, ep_ret = env.reset(), False, 0\n",
    "        while not done:\n",
    "            a = agent.act(s, epsilon)\n",
    "            s, r, done, _ = env.step(a)\n",
    "            ep_ret += r\n",
    "        returns.append(ep_ret)\n",
    "    avg_ret = np.mean(returns)\n",
    "    history.append(avg_ret)\n",
    "    print(f\"Epoch {epoch}  AvgRet {avg_ret:.4f}  ε={epsilon:.4f}\")\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# 7) PLOTTING\n",
    "# --------------------------------------------------------\n",
    "# Training curves\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(range(1,num_epochs+1), history, marker='o')\n",
    "plt.xlabel('Epoch'); plt.ylabel('Avg Eval Return')\n",
    "plt.title('BCQ Training Progress'); plt.grid(True)\n",
    "\n",
    "epsilons = [epsilon_start * np.exp(decay*s) for s in range(global_step)]\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(epsilons)\n",
    "plt.xlabel('Training Step'); plt.ylabel('Epsilon')\n",
    "plt.title('ε-Decay Schedule'); plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# 8) FINAL PORTFOLIO VS S&P 500\n",
    "# --------------------------------------------------------\n",
    "# (Your original code; now env and agent are on GPU, data already loaded.)\n",
    "sp = yf.download('^GSPC', start=start, end=end, auto_adjust=False)['Close']\\\n",
    "       .ffill().bfill().values.astype(np.float32)\n",
    "state = env.reset(); done=False\n",
    "portfolio_values = []\n",
    "while not done:\n",
    "    total = (env.holdings * env.prices_raw[env.current_step]).sum().item() + env.cash\n",
    "    portfolio_values.append(total)\n",
    "    a = agent.act(state)\n",
    "    state, _, done, _ = env.step(a)\n",
    "\n",
    "sp500_values = sp[:len(portfolio_values)]\n",
    "port_norm = np.array(portfolio_values) / portfolio_values[0]\n",
    "sp_norm   = sp500_values / sp500_values[0]\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(port_norm, label='Agent Portfolio')\n",
    "plt.plot(sp_norm,   label='S&P 500')\n",
    "plt.xlabel('Time Step'); plt.ylabel('Normalized Value')\n",
    "plt.title('Normalized Portfolio vs. S&P 500')\n",
    "plt.legend(); plt.grid(True); plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b930707-631e-4ab7-9c6b-10214c900474",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1) Download S&P 500 history over the same date range as your env\n",
    "sp = yf.download('^GSPC', start=start, end=end, auto_adjust=False)['Close'] \\\n",
    "        .ffill().bfill().values.astype(np.float32)\n",
    "\n",
    "# 2) Roll out your final policy and record only portfolio values\n",
    "state = env.reset()\n",
    "done = False\n",
    "portfolio_values = []\n",
    "\n",
    "while not done:\n",
    "    # a) compute current portfolio total value\n",
    "    total_val = (env.holdings * env.prices_raw[env.current_step]).sum() + env.cash\n",
    "    portfolio_values.append(total_val)\n",
    "\n",
    "    # b) step the environment\n",
    "    action = agent.act(state)\n",
    "    state, _, done, _ = env.step(action)\n",
    "\n",
    "# 3) Align S&P 500 series to portfolio length\n",
    "sp500_values = sp[:len(portfolio_values)]\n",
    "\n",
    "# 4) Normalize both series so they start at 1.0\n",
    "port_norm = np.array(portfolio_values) / portfolio_values[0]\n",
    "sp_norm   = np.array(sp500_values)    / sp500_values[0]\n",
    "\n",
    "# 5) Plot the comparison\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(port_norm, label='Agent Portfolio')\n",
    "plt.plot(sp_norm,   label='S&P 500')\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('Normalized Value')\n",
    "plt.title('Normalized Portfolio vs. S&P 500 Performance')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "335d3be3-ca80-4420-a29d-0b7545243fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import yfinance as yf\n",
    "from pandas_datareader import data as pdr\n",
    "\n",
    "def evaluate_agent(agent,\n",
    "                   tickers,\n",
    "                   start_date='2020-01-01',\n",
    "                   end_date=None,\n",
    "                   initial_cash=10_000,\n",
    "                   transaction_cost=1e-4,\n",
    "                   device=None):\n",
    "    \"\"\"\n",
    "    Deterministic evaluation of a trained BCQAgent in the TradingEnv.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    agent : BCQAgent\n",
    "        Your trained agent with .act(state) → action (numpy array).\n",
    "    tickers : list of str\n",
    "        List of ticker symbols to download.\n",
    "    start_date : str or datetime\n",
    "        Data start (inclusive).\n",
    "    end_date : str or datetime, optional\n",
    "        Data end (exclusive).  If None, defaults to today.\n",
    "    initial_cash : float\n",
    "        Starting cash balance.\n",
    "    transaction_cost : float\n",
    "        Per-dollar transaction cost (e.g. 1e-4).\n",
    "    device : torch.device, optional\n",
    "        To which the agent should be transferred (assumes agent is already on device).\n",
    "    \"\"\"\n",
    "    # --------------------------------------------------------------------\n",
    "    # 1) Download market data\n",
    "    # --------------------------------------------------------------------\n",
    "    price_df = yf.download(\n",
    "        tickers,\n",
    "        start=start_date,\n",
    "        end=end_date,\n",
    "        auto_adjust=False\n",
    "    )[\"Close\"].ffill().bfill()\n",
    "    prices = price_df.values.astype(np.float32)\n",
    "    dates  = price_df.index\n",
    "\n",
    "    vol_df = yf.download(\n",
    "        tickers,\n",
    "        start=start_date,\n",
    "        end=end_date,\n",
    "        auto_adjust=False\n",
    "    )[\"Volume\"].ffill().bfill()\n",
    "    vols = vol_df.values.astype(np.float32)\n",
    "\n",
    "    # dividends\n",
    "    divs_df = pd.DataFrame(index=price_df.index, columns=tickers).fillna(0.0)\n",
    "    for t in tickers:\n",
    "        d = yf.Ticker(t).dividends.tz_localize(None)\n",
    "        divs_df[t] = d.reindex(price_df.index).ffill().fillna(0.0)\n",
    "    divs = divs_df.values.astype(np.float32)\n",
    "\n",
    "    # interest rate (10-year Treasury)\n",
    "    macro = pdr.DataReader(['DGS10'], 'fred',\n",
    "                           start_date, end_date).reindex(price_df.index).ffill().bfill()\n",
    "    irate = macro['DGS10'].values.astype(np.float32)\n",
    "\n",
    "    # S&P 500 benchmark\n",
    "    sp = yf.download('^GSPC', start=start_date, end=end_date,\n",
    "                     auto_adjust=False)[\"Close\"].ffill().bfill()\n",
    "    snp = sp.values.astype(np.float32)\n",
    "\n",
    "    # --------------------------------------------------------------------\n",
    "    # 2) Build evaluation environment\n",
    "    # --------------------------------------------------------------------\n",
    "    env = TradingEnv(prices, divs, vols, irate,\n",
    "                     initial_cash=initial_cash,\n",
    "                     transaction_cost=transaction_cost)\n",
    "    state = env.reset()\n",
    "    agent.device = device  # if needed\n",
    "\n",
    "    # --------------------------------------------------------------------\n",
    "    # 3) Run one episode, collecting value & holdings history\n",
    "    # --------------------------------------------------------------------\n",
    "    portf_vals, bench_vals = [], []\n",
    "    holdings_history, dates_hist = [], []\n",
    "\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = agent.act(state)          # BCQAgent.act → numpy array\n",
    "        next_state, _, done, _ = env.step(action)\n",
    "        state = next_state\n",
    "\n",
    "        t = env.current_step\n",
    "        current_val = env.cash + (env.holdings * env.prices_raw[t]).sum()\n",
    "        portf_vals.append(current_val)\n",
    "        bench_vals.append(snp[t])\n",
    "        holdings_history.append(env.holdings.copy())\n",
    "        dates_hist.append(dates[t])\n",
    "\n",
    "    # --------------------------------------------------------------------\n",
    "    # 4) Print summary returns\n",
    "    # --------------------------------------------------------------------\n",
    "    cum_return   = (portf_vals[-1] / initial_cash - 1) * 100\n",
    "    bench_return = (bench_vals[-1] / bench_vals[0] - 1) * 100\n",
    "    print(f\"Evaluation from {dates[0].date()} to {dates[-1].date()}\")\n",
    "    print(f\"Portfolio Cumulative Return: {cum_return:.2f}%\")\n",
    "    print(f\"S&P 500 Cumulative Return: {float(bench_return):.2f}%\")\n",
    "\n",
    "    # --------------------------------------------------------------------\n",
    "    # 5) Plot normalized portfolio vs. S&P 500\n",
    "    # --------------------------------------------------------------------\n",
    "    idx = pd.to_datetime(dates_hist)\n",
    "    pf_arr    = np.array(portf_vals, dtype=np.float32)\n",
    "    bench_arr = np.array(bench_vals, dtype=np.float32)\n",
    "\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.plot(idx, pf_arr / pf_arr[0],    label='Portfolio')\n",
    "    plt.plot(idx, bench_arr / bench_arr[0], label='S&P 500')\n",
    "    plt.xlabel('Date'); plt.ylabel('Relative Value (start=1.0)')\n",
    "    plt.title('Eval: Portfolio vs. S&P 500')\n",
    "    plt.legend(); plt.tight_layout(); plt.show()\n",
    "\n",
    "    # --------------------------------------------------------------------\n",
    "    # 6) Plot individual holdings (8 panels)\n",
    "    # --------------------------------------------------------------------\n",
    "    holdings_df = pd.DataFrame(\n",
    "        np.vstack(holdings_history),\n",
    "        index=idx,\n",
    "        columns=tickers\n",
    "    )\n",
    "    num_panels        = 8\n",
    "    tickers_per_panel = int(np.ceil(len(tickers) / num_panels))\n",
    "\n",
    "    for i in range(num_panels):\n",
    "        subset = tickers[i*tickers_per_panel:(i+1)*tickers_per_panel]\n",
    "        if not subset:\n",
    "            continue\n",
    "        fig, ax = plt.subplots(figsize=(12,6))\n",
    "        holdings_df[subset].plot(ax=ax)\n",
    "        ax.set(\n",
    "            xlabel='Date',\n",
    "            ylabel='Shares Held',\n",
    "            title=f'Holdings (Group {i+1})'\n",
    "        )\n",
    "        ax.xaxis.set_major_locator(mdates.AutoDateLocator())\n",
    "        ax.xaxis.set_major_formatter(\n",
    "            mdates.ConciseDateFormatter(ax.xaxis.get_major_locator())\n",
    "        )\n",
    "        ax.legend(loc='upper left', ncol=2, fontsize='small')\n",
    "        ax.grid(True)\n",
    "        plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f630abb5-c522-47b5-9fee-f199006e7baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_agent(\n",
    "    agent=agent,\n",
    "    tickers=fewer_sp500_tickers,\n",
    "    start_date='2020-01-01',\n",
    "    end_date='2025-12-31',\n",
    "    initial_cash=10_000,\n",
    "    transaction_cost=1e-4,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb08463-9c07-4789-ac01-b762376afc5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
